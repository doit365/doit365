---
title: "ğŸ˜„ NLP - í•œêµ­ì–´ ì˜í™” ë¦¬ë·° ê°ì •ë¶„ì„"
description:
date: 2020-05-24
update: 2020-05-24
tags:
  - ml
  - nlp
---

í•œêµ­ì–´ ë°ì´í„°ì— ëŒ€í•´ì„œ í…ìŠ¤íŠ¸ ë¶„ì„ì„ í•´ë³´ì. ì•„ë˜ ë°ì´í„°ëŠ” í•œêµ­ì–´ ë¶„ì„ í•™ìŠµì„ ìœ„í•´ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆë‹¤. ì—¬ê¸°ì„œëŠ” í•œê¸€ ë¶„ì„ì„ ìœ„í•´ Konlpyë¥¼ ì‚¬ìš©í•˜ê³ , í…ì„œí”Œë¡œ ì¼€ë¼ìŠ¤ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ ë§Œë“¤ë„ë¡ í•˜ê² ë‹¤.

**ë°ì´í„°ì…‹ : Naver sentiment movie corpus** 
(ë‹¤ìš´ë¡œë“œ ë§í¬ : https://github.com/e9t/nsmc/)

NSMC ì•½ì–´ê¹Œì§€ ì‚¬ìš©í•  ì •ë„ë¡œ ë§ì´ë“¤ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° ì¸ë“¯ ì‹¶ë‹¤.

**ë°ì´í„° ì„¤ëª…**

ì˜í™” ë¦¬ë·° ì¤‘ ì˜í™”ë‹¹ 100ê°œì˜ ë¦¬ë·°ì´ê³  ì´ 200,000ê°œì˜ ë¦¬ë·°(train:15ë§Œ, test:5ë§Œ)

1ì  ~ 10ì  ê¹Œì§€ì˜ í‰ì  ì¤‘ì—ì„œ ì¤‘ë¦½ì ì¸ í‰ì (5ì ~8ì )ì„ ì œì™¸í•˜ê³  ë¶„ë¥˜ë¥¼ í•˜ì˜€ë‹¤.
- ë¶€ì • : 1ì  ~ 4ì 
- ê¸ì • : 9ì  ~ 10ì 

ì¹¼ëŒì •ë³´: id, document, label
- id: ë¦¬ë·° ì•„ì´ë””
- document: ë¦¬ë·° ë‚´ìš©
- label: ë ˆì´ë¸” (0: negative, 1: positive)

ê° íŒŒì¼ì— ëŒ€í•œ ë¦¬ë·° ê°¯ìˆ˜
- ratings.txt: All 20ë§Œ
- ratings_test.txt: 5ë§Œ
- ratings_train.txt: 15ë§Œ

ëª¨ë“  ë¦¬ë·°í…ìŠ¤íŠ¸ëŠ” 140ì ì´ë‚´ì´ê³ , ê° ê°ì • ë¶„ë¥˜ëŠ” ë™ì¼í•˜ê²Œ ìƒ˜í”Œë§ ëœë‹¤.(i.e., random guess yields 50% accuracy)
- 10ë§Œê°œì˜ ë¶€ì •ì ì¸ ë¦¬ë·°
- 10ë§Œê°œì˜ ê¸ì •ì ì¸ ë¦¬ë·°
- ì¤‘ë¦½ì ì¸ ë¦¬ë·°ëŠ” ì œì™¸

### ë°ì´í„° ì¤€ë¹„

ë‹¤ìš´ë¡œë“œ ë°›ì€ ë°ì´í„°ë¥¼ pandasë¥¼ ì´ìš©í•´ ì½ì–´ë³´ì. í•„ë“œ êµ¬ë¬¸ì´ íƒ­ìœ¼ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— `\t`ë¡œ êµ¬ë¶„ìë¥¼ ì§€ì •í•´ì£¼ì–´ì•¼ í•œë‹¤.

```python
import pandas as pd

train_df = pd.read_csv("data_naver_movie/ratings_train.txt", "\t")
test_df = pd.read_csv("data_naver_movie/ratings_test.txt", "\t")
```

### ë°ì´í„° ì „ì²˜ë¦¬

ë°ì´í„°ë¥¼ í•™ìŠµ ì‹œí‚¤ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•´ì•¼ í•˜ëŠ”ë°, Konlpyë¥¼ ì´ìš©í•´ `í˜•íƒœì†Œ ë¶„ì„ ë° í’ˆì‚¬ íƒœê¹…`ì„ í•˜ë„ë¡ í•˜ì.

ì˜ì–´ì˜ ê²½ìš° ì£¼ì–´ì§„ ë‹¨ì–´ì˜ ë¹ˆë„ë§Œì„ ì‚¬ìš©í•´ì„œ ì²˜ë¦¬í•´ë„ í¬ê²Œ ë¬¸ì œëŠ” ì—†ì§€ë§Œ í•œêµ­ì–´ëŠ” ì˜ì–´ì™€ëŠ” ë‹¬ë¦¬ ë„ì–´ì“°ê¸°ë¡œ ì˜ë¯¸ë¥¼ êµ¬ë¶„ì§“ê¸°ì—ëŠ” í•œê³„ê°€ ìˆê³ , ë¦¬ë·° íŠ¹ì„±ìƒ ë§ì¶¤ë²•ì´ë‚˜ ë„ì–´ì“°ê¸°ê°€ ì œëŒ€ë¡œ ë˜ì–´ìˆì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì •í™•í•œ ë¶„ë¥˜ë¥¼ ìœ„í•´ì„œëŠ” Konlpyë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

>KonlpyëŠ” ë„ì–´ì“°ê¸° ì•Œê³ ë¦¬ì¦˜ê³¼ ì •ê·œí™”ë¥¼ ì´ìš©í•´ì„œ ë§ì¶¤ë²•ì´ í‹€ë¦° ë¬¸ì¥ë„ ì–´ëŠ ì •ë„ ê³ ì³ì£¼ë©´ì„œ í˜•íƒœì†Œ ë¶„ì„ê³¼ í’ˆì‚¬ë¥¼ íƒœê¹…í•´ì£¼ëŠ” ì—¬ëŸ¬ í´ë˜ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤.^^!

```python
from konlpy.tag import Okt
okt = Okt()
okt.pos(u'í”ë“¤ë¦¬ëŠ” ê½ƒë“¤ ì†ì—ì„œ ë„¤ ìƒ´í‘¸í–¥ì´ ëŠê»´ì§„ê±°ì•¼')
```

    [('í”ë“¤ë¦¬ëŠ”', 'Verb'),
     ('ê½ƒ', 'Noun'),
     ('ë“¤', 'Suffix'),
     ('ì†', 'Noun'),
     ('ì—ì„œ', 'Josa'),
     ('ë„¤', 'Noun'),
     ('ìƒ´í‘¸', 'Noun'),
     ('í–¥', 'Noun'),
     ('ì´', 'Josa'),
     ('ëŠê»´ì§„ê±°ì•¼', 'Verb')]


í…ŒìŠ¤íŠ¸ ì‚¼ì•„ ê°„ë‹¨í•œ ë¬¸ì¥ì„ ë„£ê³  í™•ì¸ í•´ë³´ë©´ ì´ëŸ° í˜•íƒœë¡œ ë¶„ë¦¬ë¥¼ í•´ì£¼ëŠ” ê²ƒì„ ì•Œìˆ˜ ìˆë‹¤.

í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.

```python
def tokenize(doc):
    #í˜•íƒœì†Œì™€ í’ˆì‚¬ë¥¼ join
    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]
```
>normì€ ì •ê·œí™”, stemì€ ê·¼ì–´ë¡œ í‘œì‹œí•˜ê¸°ë¥¼ ë‚˜íƒ€ëƒ„

ë¦¬ë·°ê°€ nullì¸ ê²½ìš° ìœ„ ìœ„ í•¨ìˆ˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë‹ˆ ì‚¬ì „ì— nullê°’ í™•ì¸í•´ë³´ê³  ë¹ˆë¯¼ìì—´ë¡œ ëŒ€ì²´í•˜ì!

```python
train_df.isnull().any() #documentì— nullê°’ì´ ìˆë‹¤.
train_df['document'] = train_df['document'].fillna(''); #nullê°’ì„ ''ê°’ìœ¼ë¡œ ëŒ€ì²´

test_df.isnull().any()
test_df['document'] = test_df['document'].fillna(''); #nullê°’ì„ ''ê°’ìœ¼ë¡œ ëŒ€ì²´
```

ì´ì œ í•™ìŠµë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì €ì¥í•´ë‘ì.

```python
#tokenize ê³¼ì •ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ìˆ˜ ìˆìŒ...
train_docs = [(tokenize(row[1]), row[2]) for row in train_df.values]
test_docs = [(tokenize(row[1]), row[2]) for row in test_df.values]
```

ë¶„ì„ê²°ê³¼ê°€ ëë‚¬ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ ë°ì´í„°ê°€ ë³€í˜• ë˜ì—ˆì„ ê²ƒì´ë‹¤.

```python
print(train_docs[0])
print(test_docs[0])
```

    (['ì•„/Exclamation', 'ë”ë¹™/Noun', '../Punctuation', 'ì§„ì§œ/Noun', 'ì§œì¦ë‚˜ë‹¤/Adjective', 'ëª©ì†Œë¦¬/Noun'], 0)
    (['êµ³ë‹¤/Adjective', 'ã…‹/KoreanParticle'], 1)
    

15ë§Œ í•™ìŠµë°ì´í„°ì— ë¶„ë¦¬ëœ í† í° ê°œìˆ˜ë¥¼ ì‚´í´ë³´ì.

```python
tokens = [t for d in train_docs for t in d[0]]
print("í† í°ê°œìˆ˜:", len(tokens))
```

    í† í°ê°œìˆ˜: 2159921
    
ì´ì œ ì´ë°ì´í„°ë¥¼ ê°€ì§€ê³  nltkë¥¼ ì´ìš©í•´ ì „ì²˜ë¦¬ë¥¼ í•œë‹¤. `Text` í´ë˜ìŠ¤ëŠ” ë¬¸ì„œë¥¼ í¸ë¦¬í•˜ê²Œ íƒìƒ‰í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ê³  ìˆë‹¤.

ì—¬ê¸°ì„œëŠ” `vocab().most_common` ë§¤ì„œë“œë¥¼ ì´ìš©í•´ ë°ì´í„°ê°€ ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ë¥¼ ê°€ì ¸ì˜¬ ë•Œ ì‚¬ìš©í•˜ê² ë‹¤.

```python
import nltk
text = nltk.Text(tokens, name='NMSC')

#í† í°ê°œìˆ˜
print(len(text.tokens))

#ì¤‘ë³µì„ ì œì™¸í•œ í† í°ê°œìˆ˜
print(len(set(text.tokens)))

#ì¶œë ¥ë¹ˆë„ê°€ ë†’ì€ ìƒìœ„ í† í° 10ê°œ
print(text.vocab().most_common(10))
```

    2159921
    49894
    [('./Punctuation', 67778), ('ì˜í™”/Noun', 50818), ('í•˜ë‹¤/Verb', 41209), ('ì´/Josa', 38540), ('ë³´ë‹¤/Verb', 38538), ('ì˜/Josa', 30188), ('../Punctuation', 29055), ('ê°€/Josa', 26627), ('ì—/Josa', 26468), ('ì„/Josa', 23118)]
    

### ë°ì´í„° íƒìƒ‰

ì¶œë ¥ë¹ˆë„ê°€ ë†’ì€ ìƒìœ„ í† í° 10ê°œë¥¼ matplotlibì„ ì´ìš©í•´ ê·¸ë˜í”„ë¡œ í™•ì¸í•´ë³´ì.

```python
%matplotlib inline
```

```python
import matplotlib.pyplot as plt
from matplotlib import font_manager, rc
plt.figure(figsize=(20,10))
text.plot(50)
```

![png](korea_movie_review_plot.png)


ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ë°±í„°í™”ë¥¼ í•´ì•¼ í•˜ëŠ”ë°, ìì£¼ ì‚¬ìš©ë˜ëŠ” í† í° 10000ê°œë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ë°±í„°í™” í•˜ì.(ì› í•« ì¸ì½”ë”© ëŒ€ì‹  CountVectorizationì„ ì‚¬ìš©)

ë¬¸ì„œ ì§‘í•©ì—ì„œ ë‹¨ì–´ í† í°ì„ ìƒì„±í•˜ê³  ê° ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì„¸ì–´ BOW(Bag of Words) ì¸ì½”ë”©í•œ ë²¡í„°ë¥¼ ë§Œë“œëŠ” ì—­í• ì„ í•œë‹¤.

ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ 100ê°œë§Œ í•´ë³´ì...


```python
FREQUENCY_COUNT = 100; #ì‹œê°„ì  ì—¬ìœ ê°€ ìˆë‹¤ë©´ 10000ê°œë¥¼ í•´ë³´ë„ë¡~
```

```python
selected_words = [f[0] for f in text.vocab().most_common(FREQUENCY_COUNT)]
```

ì´ ê³¼ì •ì€ ë°ì´í„° ì–‘ì´ í° ë§Œí¼ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— ì´ ì‘ì—…ì„ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ íƒœê¹…ì„ ë§ˆì¹œ í›„ì—ëŠ” jsoníŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë‹¤.

ë¬¸ì„œì—ì„œ ìƒìœ„ë¡œ ì„ íƒëœ ë‹¨ì–´ë“¤ì¤‘ ëª‡ê°œê°€ í¬í•¨ì´ ë˜ëŠ”ì§€ë¥¼ ì•Œì•„ì•¼ í•œë‹¤.

```python
#ë‹¨ì–´ë¦¬ìŠ¤íŠ¸ ë¬¸ì„œì—ì„œ ìƒìœ„ 10000ê°œë“¤ì¤‘ í¬í•¨ë˜ëŠ” ë‹¨ì–´ë“¤ì´ ê°œìˆ˜
def term_frequency(doc):
    return [doc.count(word) for word in selected_words]
```

```python
#ë¬¸ì„œì— ë“¤ì–´ê°€ëŠ” ë‹¨ì–´ ê°œìˆ˜
x_train = [term_frequency(d) for d,_ in train_docs]
x_test = [term_frequency(d) for d,_ in test_docs]
```

```python
#ë¼ë²¨(1 or 0)
y_train = [c for _,c in train_docs]
y_test = [c for _,c in test_docs]
```

ì´ë ‡ê²Œ í•˜ë©´ xì¶• ë°ì´í„°ì—ëŠ” ë‹¨ì–´ë“¤ì´ ë¹ˆë„ìˆ˜ ì •ë³´?, yì¶•ì—ëŠ” ë¶„ë¥˜ ê²°ê³¼ë¥¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.

ì´ì œ ë°ì´í„°ë¥¼ floatë¡œ í˜• ë³€í™˜ ì‹œì¼œì£¼ë©´ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì€ ë~~

```python
x_train = np.asarray(x_train).astype('float32')
x_test = np.asarray(x_test).astype('float32')

y_train = np.asarray(y_train).astype('float32')
y_test = np.asarray(y_test).astype('float32')
```

### ë°ì´í„° ëª¨ë¸ë§

í…ì„œí”Œë¡œ ì¼€ë¼ìŠ¤ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì.

ë ˆì´ì–´ êµ¬ì„±ì€ ë‘ê°œì˜ Danseì¸µì€ 64ê°œì˜ ìœ ë‹›ì„ ê°€ì§€ê³  í™œì„±í•¨ìˆ˜ëŠ” reluë¥¼ ì‚¬ìš©í•˜ê³ , ë§ˆì§€ë§‰ì¸µì€ sigmoid í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê¸ì • ë¦¬ë·°ì¼ í™•ë¥ ì„ ì¶œë ¥í•  ê²ƒì´ë‹¤.

```python
import tensorflow as tf

#ë ˆì´ì–´ êµ¬ì„±
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(FREQUENCY_COUNT,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

ì†ì‹¤ í•¨ìˆ˜ëŠ” binary_crossentropy, RMSprop ì˜µí‹°ë§ˆì´ì €ë¥¼ í†µí•´ ê²½ì‚¬í•˜ê°•ë²•ì„ ì§„í–‰

```python
#í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì„¤ì •
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),
    loss=tf.keras.losses.binary_crossentropy,
    metrics=[tf.keras.metrics.binary_accuracy]
    )
```

ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” 512, ì—í¬í¬ëŠ” 10ë²ˆìœ¼ë¡œ í•™ìŠµ

ì, ì´ì œ í•™ìŠµì„ ì‹œì¼œ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì! ë¨¼ê°€ ìˆì–´ ë³´ì´ëŠ” ì§„í–‰ë¥  ìƒíƒœë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.:)

```python
#í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµ
model.fit(x_train, y_train, epochs=10, batch_size=512)
```

    WARNING:tensorflow:From C:\Users\DESKTOP\.conda\envs\nlp\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Epoch 1/10
    150000/150000 [==============================] - 17s 115us/sample - loss: 0.5611 - binary_accuracy: 0.6948s - loss: 0.6134 - binary_accuracy:  - ETA: 21s - loss: 0.6108 - binary_accuracy: 0. - ETA: 20s -  - ETA: 15s - loss: 0.5957 - binary_accuracy: 0.67 - ETA: 15s - loss: 0.5955 - binary_accuracy: 0.67 - ETA: 15s - loss: 0.5951 - binary_accura - ETA: 14s - loss: 0.5930 - binary_accuracy: 0. - ETA: 14s - loss: 0.5923 - binary_accuracy:  - ETA: 13s - loss: 0.5911 - binary_accura - ETA: 8s - loss: 0.5785 - binary_accuracy: 0. - ETA: 8s - loss: 0.5771 - binary_accuracy: 0.68 - ETA: 8s - loss: 0.5764 - binary_accuracy:  - ETA: 7s - loss: 0.5751 - binary_accura - ETA: 6s - loss: 0.5727 - binary_accuracy: - ETA: 6s - loss: 0.5713 - binary_accuracy: 0.6 - ETA: 5s - loss: 0.5708 - binary_a - ETA: 4s - loss: 0.5685 - binary_accuracy: - ETA: 4s - loss: 0.5676 - binary_accuracy: 0.691 - ETA: 4s - loss: 0.5674 - binary_a - ETA: 2s - loss: 0.5652 - binary_accuracy: 0.692 - ETA: 2s - loss: 0.5651 - binary_accurac - ETA: 1s - loss: 0.5639 - binary_accuracy - ETA: 1s - loss: 0.5628 - binary_accuracy: 0.694 - ETA: 1s - loss: 0.5627 - binary_accuracy: 0.694 - ETA: 1s - loss: 0.5626 - binary_accuracy: 0.694 - ETA: 1s - loss: 0.5623 - binary_accuracy: 0.694 - ETA: 1s - loss: 0.5623 - binary_accuracy:  - ETA: 0s - loss: 0.5615 - binary_accuracy: 0.69 - ETA: 0s - loss: 0.5612 - binary_accuracy: 0.694 - ETA: 0s - loss: 0.5613 - binary_accuracy: 0.6
    Epoch 2/10
    150000/150000 [==============================] - 12s 83us/sample - loss: 0.5313 - binary_accuracy: 0.71254s - loss: 0.5221 - binary_ac - ETA: 15s - loss: 0.5345 - binary_accuracy:  - ETA: 16s - loss: 0.5377 - binary_accuracy - ETA: 14s - loss - ETA: 13s - lo - ETA: 7s - loss: 0.5341 - binary_accuracy: 0.71 - ETA: 7s -  - ETA: 4s - loss: 0.5340 - binary_accur - ETA: 3s - loss: 0.5335 - binary_accuracy: 0.71 - ETA: 3s - loss: 0.5335 - binary_ - ETA: 2s - loss: 0.5325 - binary_accuracy: 0 - ETA: 1s - loss: 0.
    Epoch 3/10
    150000/150000 [==============================] - 13s 86us/sample - loss: 0.5236 - binary_accuracy: 0.7170s - loss: 0.5284 - binary_accuracy - ETA: 10s - los - ETA: 9s - loss: 0.5255 - binary_accurac - ETA: 9s - loss: 0.5250 - binary_accuracy: 0. - ETA: 9s - loss: 0.5255 - binary_accuracy: 0.716 - ETA: 9s - loss: 0.5254 - binary - ETA: 8s - loss: 0.5247 - binary_acc - ETA: 7s - loss: 0.5251 - binary_accuracy: 0.7 - ETA: 7s - loss: 0.5252 - binary_accuracy: 0.71 - ETA: 7s - loss: 0.5249 - binary_accuracy: 0.716 - ETA: 7s - loss: 0.5248 - binary_accuracy: 0.7 - ETA: 6s - loss: 0.5245 - binary_accuracy: - ETA: 6s - loss: 0.5246 - binary_accuracy: 0 - ETA: 5s - loss: 0.5247 - bina - ETA: 5s - loss: 0.5249 - b - ETA: 3s - loss: 0.5246 - binary_ac - ETA: 2s - loss: 0.5246 - binary_ac - ETA: 2s - loss: 0.5244 - binary_accuracy: 0.71 - ETA: 2s - loss: 0.5244 - binary_accuracy: - ETA: 1s - loss: 0.5242 - binary_accur - ETA: 1s - loss: 0.5243 - binary_accuracy: 0 - ETA: 0s - loss: 0.5243 - binary_accuracy - ETA: 0s - loss: 0.5239 - binary_accuracy: 0.7 - ETA: 0s - loss: 0.5237 - binary_accuracy: 0.7 - ETA: 0s - loss: 0.5236 - binary_accuracy: 0.71
    Epoch 4/10
    150000/150000 [==============================] - 13s 89us/sample - loss: 0.5179 - binary_accuracy: 0.7219s - loss: 0.5201 - binary_accuracy: 0 - ETA: 8s - loss: 0.5211 - binary_a - ETA: 9s - loss: 0.5208 - binary_accuracy: 0.721 - ETA: 8s - loss: 0.5201 - binary_accuracy: 0.72 - ETA: 8s - loss: 0.5210 - - ETA: 7s - loss: 0.5180 - binary_ac - ETA: 7s - loss: 0.5187 - binar - ETA: 6s - loss: 0.5186 - binary_accura - ETA: 6s - loss: 0.5183 - binary_accuracy: 0.7 - ETA: 6s - loss: 0.5182 - binary_ac - ETA: 5s - loss: 0.5179 - binary_accuracy: 0.723 - ETA: 5s - loss: 0.5179 - binary_accurac - ETA: 4s - loss: 0.5187 - binary_accuracy: 0.722 - ETA: 4s - lo
    Epoch 5/10
    150000/150000 [==============================] - 13s 87us/sample - loss: 0.5132 - binary_accuracy: 0.72531s - loss: 0.5093 - binary_accuracy: 0.72 - ETA: 11s - ETA: 9s - loss: 0.5156 - binary_accuracy: 0.7 - ETA: 9s - loss: 0.5163 - binary_accuracy:  - ETA: 9s - loss: 0.5154 - binary_accurac - ETA: 8s - loss: 0.5164 - bin - ETA: 7s - loss: 0.5156 - binary - ETA: 6s - loss: 0.5168 - binary_accuracy: 0. - ETA: 6s - loss: 0.5163 - binary_accuracy: 0.72 - ETA: 6s - loss: 0.5160 - binary_accur - ETA: 5s - loss: 0.5154 - binary_accuracy: 0.723 - ETA: 5s - loss: 0.5153 - binary_a - ETA: 4s - los - ETA: 2s - loss: 0.5136 - binary_acc - ETA: 1s - loss: 0.5135 - binary_accuracy: 0.724 - ETA: 1s - loss: 0.5134 - binary_accuracy - ETA: 1s - loss: 0.5132 - binary_ac
    Epoch 6/10
    150000/150000 [==============================] - 13s 87us/sample - loss: 0.5094 - binary_accuracy: 0.72850s - loss: 0.4971 - binary_accuracy: 0.73 - ETA: 11s - loss: 0.5000 - binary_ - ETA: 9s - loss: 0.5043 - binary_accuracy: - ETA: 10s - loss: 0.5081 - binary_accuracy:  - ETA: 9s - loss: 0.5086 - binary_a - ETA: 9s - loss: 0.5106 - binary_accuracy: 0 - ETA: 9s - loss: 0.5111 - binary_accuracy: 0.72 - ETA: 9s - loss: 0.5112 - binary_accuracy:  - ETA: 9s - loss: 0.5122 - binary_accuracy: 0.724 - ETA: 9s - loss: 0.5122 - binary_accuracy:  - ETA: 8s - loss: 0.5114 - binary_accuracy: 0. - ETA: 8s - loss: 0.5115 - binary_accuracy: 0. - ETA: 8s - loss: 0.5118 - binary_accuracy: 0 - ETA: 8s - loss: 0.5111 - binary_accuracy: 0.725 - ETA: 8s - loss: 0.5114 - - ETA: 6s - loss: 0.5099 - binary_accuracy - ETA: 6s - loss: 0.5108 - binary_accuracy - ETA: 5s - loss: 0.5100 - binary_accuracy: 0.726 - ETA: 5s - loss: 0.5102 - binary_acc - ETA: 4s - loss: 0.5102 - binary_accurac - ETA: 4s - loss: 0.5102 - binary_accuracy: 0.7 - ETA: 4s - l - ETA: 1s - loss: 0.5102 - binary_accu - ETA: 1s - loss: 0.5098 - bi - ETA: 0s - loss: 0.5094 - binary_accuracy: 0.72
    Epoch 7/10
    150000/150000 [==============================] - 12s 79us/sample - loss: 0.5064 - binary_accuracy: 0.73061s - loss: 0.5050 - binary_accura - ETA: 9s - loss: 0.5096 - binary_accuracy: 0.72 - ETA: 9s - loss: 0.5090 - binary_accuracy - ETA: 8s - loss: 0.5083 - binary_accuracy: 0.7 - ETA: 8s - loss: 0.5082 - binary_accuracy: 0.7 - ETA: 8s - loss: 0.5083 - binary_accur - ETA: 7s - loss: 0.5085 - binary_accuracy: 0 - ETA: 6s - loss: 0.5079 - binary_a - ETA: 5s - loss: 0.5079 - binary_accuracy: 0. - ETA: 5s - loss: 0.5080 - binary_accuracy:  - ETA: 5s - loss: 0.5078 - binary_accuracy - ETA: 4s - loss: 0.5081 - binary_accuracy: 0 - ETA: 4s - loss: 0.5080 - binary_accuracy: 0.730 - ETA: 4s - loss: 0.5080 - binary_accuracy: 0.730 - ETA: 4s - loss: 0.5078 - binary_accuracy:  - ETA: 4s - loss: 0.5072 - binary_accuracy: 0 - ETA: 3s - loss: 0.5072 - binary_ - ETA: 2s - lo - ETA: 0s - loss: 0.5064 - binary_accuracy: 0.730
    Epoch 8/10
    150000/150000 [==============================] - 13s 85us/sample - loss: 0.5037 - binary_accuracy: 0.73210s - loss: 0.5053 - binary_acc - ETA: 9s - loss: 0.5045 - binary_accuracy:  - ETA: 9s - loss: 0.5024 - binary_accu - ETA: 8s - loss: 0.5013 - binary_accu - ETA: 8s - loss: 0.5014 - binary_accuracy: 0.73 - ETA: 8s - loss: 0.5007 - binar - ETA: 6s - loss: 0.5013 - binary_a - ETA: 6s - loss: 0.5016 - binary_accuracy:  - ETA: 6s - loss: 0.5019 - binary_accuracy: 0.73 - ETA: 5s - loss: 0.5019 - binary_accuracy: 0 - ETA: 5s - loss: 0.5023 - binary_accuracy: 0.73 - ETA: 5s - loss: 0.5021 - binary_accuracy: 0.73 - ETA: 5s - l - ETA: 3s - loss: 0.5027 - ETA: 2s - loss: 0.5036 - binary_accuracy:  - ETA: 2s - loss: 0.5033 - binary_accuracy: 0.732 - ETA: 2s - loss: 0.5033 - binary_accuracy: 0.73 - ETA: 2s - loss: 0.5034 - binary_accur - ETA: 1s - loss: 0.5035 - binary_accuracy:  - ETA: 0s - loss: 0.5033 - binary_accuracy:  - ETA: 0s - loss: 0.5035 - binary_acc
    Epoch 9/10
    150000/150000 [==============================] - 13s 85us/sample - loss: 0.5015 - binary_accuracy: 0.7337s - loss: 0.5023 - binary_accu - ETA: 13s - loss: 0.4956 - binary_accuracy: 0. - ETA: 14s - loss: 0.4949 - binary_accu - ETA: 15s - loss: 0.4938 - binary_accuracy: 0. - ETA: 15s - loss: 0.4977 - binary_accuracy: 0.73 - ETA: 15s - loss: 0.4981 - binary_accuracy: 0. - ETA: 14s - loss:  - ETA: 12s - loss: 0.4989 - binary_accuracy - ETA: 12s - loss: 0.4992 - binary_accuracy - ETA: 11s - loss: 0.4990 - binary_ - ETA: 10s - loss: 0.4982 - binary_accuracy: 0.73 - ETA: 10s - loss: 0.4982 - binar - ETA: 9s - loss: 0.4997 - binary_accuracy: 0. - ETA: 9s - loss: 0.4997 - binary_accuracy: 0 - ETA: 8s - loss: 0.4996 - binary_accuracy: 0.7 - ETA: 8s - loss: 0.4997 - binary - ETA: 7s - loss: 0.5011 - bina - ETA: 6s - loss: 0.5014 - binary_accuracy: 0.7 - ETA: 6s - loss: 0.5011 - binary_ - ETA: 4s - loss: 0.5016 - binary_accuracy: 0.733 - ETA: 4s - loss: 0.5017 - binary_accurac - ETA: 4s - loss: 0.5018 - binary_ac - ETA: 2s - loss: 0.5022 - binary_accuracy: 0.7 - ETA: 2s - loss: 0.5023 - binary_accuracy: 0.73 - ETA: 2s - loss: 0.5021 - binary_accuracy: - ETA: 1s - loss: 0.5018 - binary_accuracy: 0 - ETA: 1s - loss: 0.5019 - binary_accuracy - ETA: 1s - loss: 0.5018 - binary_accuracy: 0.73 - ETA: 1s - loss: 0.5018 - binary_accuracy: 0.7 - ETA: 1s - loss: 0.5017 - binary_a
    Epoch 10/10
    150000/150000 [==============================] - 14s 91us/sample - loss: 0.4995 - binary_accuracy: 0.73620s - loss: 0.4948 - binary_accuracy:  - ETA: 14s - loss: 0.4965 - binary_accura - ETA: 14s - loss: 0.4980 - binary_accu - ETA: 12s - loss: 0.4973 - binary_accuracy - ETA: 12s - loss: 0.5014 - binar - ETA: 12s - loss: 0.4992 - binary_accuracy - ETA: 12s - loss: 0.4996 - binary_accuracy:  - ETA: 12s - loss: 0.5001 - binary_accuracy: 0.73 - ETA: 12s - loss: 0.5003 - binary_accura - ETA: 11s - loss: 0.5011 - b - ETA: 9s - loss: 0.5027 - binary_accuracy:  - ETA: 9s - loss: 0.5026 - binary_accuracy - ETA: 8s - loss: 0.5013 - binary_accur - ETA: 8s - loss:  - ETA: 6s - loss: 0.5013 - binary_ - ETA: 5s - loss: 0.5006 - binary_ac - ETA: 4s - loss: 0.4997 - binary_accurac - ETA: 4s - loss: 0.5001 - binary_ac - ETA: 3s - loss: 0.5001 - b - ETA: 1s - loss: 0.4989 - binary_accuracy: 0.736 - ETA: 1s - loss: 0.4989 - binary_accuracy: 0.736 - ETA: 1s - loss: 0.4988 - binary_accuracy: 0.736 - ETA: 1s - loss: 0.4989 - binary_accurac - ETA: 1s - loss: 0.4991 - binary_accuracy: 0. - ETA: 0s - loss: 0.4993 - binary_accura - ETA: 0s - loss: 0.4995 - binary_accuracy: 0.73
    
    <tensorflow.python.keras.callbacks.History at 0x1967af62ac8>



### ëª¨ë¸ í‰ê°€

í•™ìŠµë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ í•™ìŠµì´ ëë‚¬ë‹¤ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ëª¨ë¸ì„ í‰ê°€í•´ë³´ì.

```python
results = model.evaluate(x_test, y_test)
```

    50000/50000 [==============================] - 12s 234us/sample - loss: 0.5198 - binary_accuracy: 0.7184s - loss: 0.5197 - b - ETA: 11s - loss: 0.5199 - binary_accuracy: 0.  - ETA: 1s - loss: 0.5198 - b
    

```python
#loss: 0.5, acc: 0.7
results
```

    [0.5197769028568268, 0.71842]


>ì—¬ê¸°ì„œëŠ” 100ê±´ìœ¼ë¡œ í–ˆê¸°ë•Œë¬¸ì— ì¢€ ë‚®ì€ 71%ì˜ ì •í™•ë„ê°€ ë‚˜ì™”ë‹¤. ì•„ë§ˆ ì‚¬ìš©í•œ í† í°ìˆ˜ë¥¼ 100ê°œê°€ ì•„ë‹Œ 10000ê°œë¡œ í–ˆë‹¤ë©´ 85%ì •ë„ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

íŒìœ¼ë¡œ í˜ë“¤ê²Œ ë§Œë“  ëª¨ë¸ì„ ì•„ë˜ì™€ ê°™ì´ ì €ì¥í•´ë‘ê³  ë‚˜ì¤‘ì— ë¡œë“œí•´ì„œ ì‚¬ìš©í• ìˆ˜ ìˆìœ¼ë‹ˆ ê¼­ ì•Œì•„ë‘ì.

```python
#ëª¨ë¸ì„ ì €ì¥í•´ë‘˜ìˆ˜ë„ ìˆë‹¤.
model.save('movie_review_model.h5')

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
#from keras.models import load_model
#model = load_model('movie_review_model.h5')
```

### ê²°ê³¼ ì˜ˆì¸¡í•˜ê¸°

ì´ì œ ë¦¬ë·° ë¬¸ìì—´ì„ ë°›ì•„ ë°”ë¡œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ì

ë°ì´í„° í˜•íƒœë¥¼ ë§ì¶”ê¸° ìœ„í•´ np.expand_dims ë§¤ì„œë“œë¥¼ ì´ìš©í•´ arrayì˜ ì¶•ì„ í™•ì¥ ì‹œì¼œì£¼ì–´ì•¼ í•œë‹¤.

ìµœì¢… í™•ë¥ ì´ 0.5 ì´ìƒì´ë©´ ê¸ì •, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¶€ì •ì´ë¼ê³  ì˜ˆì¸¡ì„ í•˜ê² ë‹¤.

ëŒ€ëµ í…ŒìŠ¤íŠ¸ë¥¼ ë¨¼ì € í•´ë³´ê³ ...

```python
review = "ì•„ì£¼ ì¬ë¯¸ ìˆì–´ìš”"
token = tokenize(review)
token
```

    ['ì•„ì£¼/Noun', 'ì¬ë¯¸/Noun', 'ìˆë‹¤/Adjective']


```python
tf = term_frequency(token)
```

```python
data = np.expand_dims(np.asarray(tf).astype('float32'), axis=0)
```

```python
float(model.predict(data))
```

    0.9102853536605835


í…ŒìŠ¤íŠ¸í•œ ë¡œì§ì„ í•¨ìˆ˜í™”í•´ì„œ ì‚¬ìš©í•˜ì.

```python
def predict_review(review):
    token = tokenize(review)
    tfq = term_frequency(token)
    data = np.expand_dims(np.asarray(tfq).astype('float32'), axis=0)
    score = float(model.predict(data))
    if(score > 0.5):
        print(f"{review} ==> ê¸ì • ({round(score*100)}%)")
    else:
        print(f"{review} ==> ë¶€ì • ({round((1-score)*100)}%)")
```


```python
predict_review("ì¬ë¯¸ ì •ë§ ì—†ì–´ìš”")
```

    ì¬ë¯¸ ì •ë§ ì—†ì–´ìš” ==> ë¶€ì • (93%)

ì´ì œ ë¦¬ë·°í…ìŠ¤íŠ¸ ë§Œìœ¼ë¡œ ê¸ì •ì¸ì§€ í˜¹ì€ ë¶€ì •ì¸ì§€ë¥¼ ì–´ëŠì •ë„ íŒë‹¨ í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

ì§€ê¸ˆê¹Œì§€ ì˜í™”ë¦¬ë·° ë°ì´í„°ë¥¼ í†µí•´ì„œ ê°ì •ë¶„ì„ì„ í•´ë³´ì•˜ëŠ”ë° ìƒí’ˆ, ê²Œì„, ìŒì‹ë“±ì˜ ì‚¬ìš©ì ì˜ê²¬ì´ ë‹´ê¸´ ë°ì´í„°ë¥¼ ì˜ ëª¨ì•„ì„œ í™œìš©í•œë‹¤ë©´ ë‹¤ì–‘í•œ ê³³ì— í™œìš©í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.